# C++ RPC Framework

一个功能完整、高性能的C++ RPC框架，支持服务注册发现、负载均衡、异步调用等特性。

## 🎯 整体架构

```
┌──────────────────────────────────────────────────────────────────┐
│                         RPC Framework                             │
│                                                                   │
│  ┌─────────────┐         ┌──────────────┐        ┌────────────┐ │
│  │   Client    │◄───────►│  ZooKeeper   │◄───────►│  Server    │ │
│  │             │         │   Registry   │         │            │ │
│  │ calculator_ │         │  (Service    │         │ calculator_│ │
│  │  client     │         │  Discovery)  │         │  server    │ │
│  └──────┬──────┘         └──────────────┘         └──────┬─────┘ │
│         │                                                 │       │
│         │         Direct RPC Call (TCP)                   │       │
│         └─────────────────────────────────────────────────┘       │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘
```

## 🏗️ 项目学习阶段

### 1.链路层 (Link Layer)：整个系统的基石。
目标: 实现一个基于 epoll 的高性能 TCP 服务，能够高效地接收和发送二进制数据。
学习重点: 序列化与反序列化 (Protobuf)，TcpServer 类设计与实现。socket 编程、非阻塞 I/O、epoll 原理与实践。

### 2.协议层 (Protocol Layer)：RPC 系统可以自定义的协议来规范数据格式、或者使用 Protobuf 封装协议。
目标: 定义一个 RPC 协议头，包含请求的类型、方法名、参数长度等元信息。
学习重点: 网络协议设计、报文分包与组包。

### 3.序列化层 (Serialization Layer)：将对象转换为二进制数据，这部分我们已经初步完成了。
目标: 封装 Protobuf，使其能更方便地与协议层对接。

### 4.代理层 (Proxy Layer)：这是 RPC 系统的“门面”，客户端通过代理层像调用本地函数一样调用远程函数。
目标: 使用 C++ 模板和宏来生成代理类，实现对远程调用的封装。
学习重点: C++ 模板元编程、宏的高级用法。

### 5.路由层 (Routing Layer)：客户端如何知道去哪台服务器发起调用？
目标: 实现一个客户端路由策略，比如简单的负载均衡。
学习重点: 负载均衡算法（如轮询、随机）。

### 6.注册中心层 (Registry Center Layer)：服务端如何被发现？
目标: 实现一个简单的服务注册与发现机制，让服务提供者能注册自己，服务消费者能找到它们。
学习重点: 分布式系统原理、服务注册与发现模型。

### 7.容错层 (Fault Tolerance Layer)：当服务出现故障时，系统如何应对？
目标: 实现超时、重试等容错机制，提高系统的健壮性。
学习重点: 分布式系统中的容错设计。



## 🚀 特性

- **高性能传输层**: 基于epoll的非阻塞TCP服务器
- **协议缓冲**: 使用Google Protocol Buffers进行序列化
- **服务注册发现**: 使用Zookeeper支持服务注册和动态发现
- **负载均衡**: 支持轮询、随机、加权轮询等策略
- **异步调用**: 支持同步和异步RPC调用
- **线程池**: 内置线程池处理并发请求
- **模块化设计**: 清晰的架构分层，易于扩展

## mini_zookeeper
包含以下内容：
Registry:注册中心
Provider:服务提供者
Consumer:服务消费者

```
zookeeper注册中心，应包含以下内容：
 1. 注册服务 register(service_name, service_address)
 2. 发现服务 find(service_name)
 3. 移除服务 remove(service_name, service_address)
 4. 心跳机制 服务提供者定期向注册中心发送心跳，证明自己还活着
 
 服务注册、发现、移除:可以用map<name, vector<service>> 来保存服务实例
 心跳机制:服务提供者通过网络定时向注册中心发送一个数据包，注册中心更新服务实例的时间信息。注册中心还要定期检查服务提供者有没有超时，将超时的服务移除。
 服务提供者向注册中心发送的心跳包，要包含哪些信息？   --- 提供的服务名称、自己的地址。
 注册中心收到心跳包，怎么处理？跟踪哪些状态？        --- 通过服务名称和地址，找到已注册的实例，更新它的时间信息
 注册中心如何判断一个服务提供者是否超时？           --- 注册中心要定时轮询，检查服务实例的发送时间，是否已经超时
 服务提供者确认下线了，如何处理？                  --- 从vector中移除
 
 如何实现定时检查？ --- 1. 另起一个独立线程，负责检查实例是否超时，让这个线程循环休眠、检查。 2. epoll_wait + 定时器。
```

## 高并发网络通信
### 传统的`socket`编程模型，在高并发场景下有什么问题？epoll为什么适合高并发场景？
在传统的`socket`编程模型中，`accept()` `read()` 函数是阻塞的，如果没有新连接或数据，线程就会被挂起/阻塞，直到有事件发生。
在多线程/多进程模型中：每来一个新连接，就会创建一个线程或进程处理它，这样可以解决阻塞问题，但是：
 - 资源消耗：每个线程/进程都需要消耗内存和CPU。高并发场景下，资源很快消耗殆尽。
 - 上下文切换：线程/进程之间频繁的切换会带来额外的开销，影响整体性能。

`epoll`如何解决痛点？
epoll是事件驱动(Event-Driven)的I/O模型。我们不再需要死等某个连接的事件，epoll会告诉内核，让内核帮忙监听感兴趣的事件，事件来了再通知我。
可以在一个单线程里，通过`epoll_wait`调用，高效地等待多个`socket`上的事件，事件发生时，`epoll_wait`会立刻返回，告诉我们哪些`socket`准备好了。
这样，可以避免创建大量的线程，降低了资源的消耗，也减少了上下文切换的开销。

`epoll`相比于`select``poll`这两个多路复用的系统调用，有哪些优势？
 - 就绪事件：`select``poll`在有事件就绪后，会返回一个大集合，需要我们遍历查找就绪的事件。`epoll`会返回一个已就绪事件的列表。
 - 共享内存：`select``poll`在每次调用时，都要讲感兴趣的`socket`集合从用户态复制到内核态，开销很大。`epoll`一次性注册事件，底层使用红黑树和就绪列表，每次调用共享内存。